{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4DbfhuC0kfik"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gdown'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ACER\\Documents\\Documents\\Leon Folder\\ML-KYC-Feature\\imageclassification_1.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ACER/Documents/Documents/Leon%20Folder/ML-KYC-Feature/imageclassification_1.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ACER/Documents/Documents/Leon%20Folder/ML-KYC-Feature/imageclassification_1.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgdown\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ACER/Documents/Documents/Leon%20Folder/ML-KYC-Feature/imageclassification_1.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtarfile\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ACER/Documents/Documents/Leon%20Folder/ML-KYC-Feature/imageclassification_1.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mzipfile\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gdown'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gdown\n",
        "import tarfile\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of anime characters: 46\n"
          ]
        }
      ],
      "source": [
        "# List of anime character names\n",
        "anime_characters = ['amelia_watson', 'Anya_Forger', 'Aquamarine_Hoshino', 'arima_kousei', 'Ayaka_Genshin_Impact', 'Boa_Hancock', \n",
        "                   'Charlotte_Genshin_Impact', 'Damian_Desmond', 'Dazai_Osamu_BSD', 'fern', 'frieren', 'Ganyu_genshin', 'gawr_gura', 'Gojo_Satoru', \n",
        "                   'hoshino_ai', 'hutao_genshin', 'Jett_Valorant', 'Kafka_Honkai_Star_Rail', 'kaori', 'Keqing_genshin', 'Killjoy_Valorant', \n",
        "                   'Kobo_kanaeru', 'Kugisaki_Nobara', 'Loid_Forger', 'Luffy_D_Monkey', 'Midoriya_Izuku', 'mikasa', 'Minato_Aqua', \n",
        "                   'Misa_Amane', 'Mitsuri_Kanroji', 'Nico_Robin', 'Nier_Automata_9S', 'Nier_Automata_A2', 'Raiden_Shogun_Genshin_Impact'\n",
        "                   , 'Ruby_Hoshino', 'Sakura_Haruno', 'tanjiro', 'Todoroki_Shoto', 'Tokisaki_Kurumi', 'Uraraka_Ochako', \n",
        "                   'violet_evergarden', 'Wanderer', 'Yor_Forger', 'Yuri_Briar', 'Zerotwo', 'zeta_hololive']\n",
        "\n",
        "# Count the number of characters\n",
        "num_characters = len(anime_characters)\n",
        "\n",
        "# Print the result\n",
        "print(\"Number of anime characters:\", num_characters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5mN0yyC_7FW8"
      },
      "outputs": [],
      "source": [
        "file_id = '15M-lNTPksPL3RusWIaM0wsgNTHhrPxif'\n",
        "file_url = f'https://drive.google.com/uc?id={file_id}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-Ozodzi17RXr"
      },
      "outputs": [],
      "source": [
        "destination_dir = '/content/images'\n",
        "os.makedirs(destination_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "PYqJGCEC7VRZ",
        "outputId": "db5c0c66-edcb-4f5d-d72d-e451cc4ac661"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15M-lNTPksPL3RusWIaM0wsgNTHhrPxif\n",
            "To: /content/images/images.zip\n",
            "100%|██████████| 258M/258M [00:04<00:00, 58.5MB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/images/images.zip'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download the file\n",
        "zip_file_path = os.path.join(destination_dir, 'images.zip')\n",
        "gdown.download(file_url, zip_file_path, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ow9WYZs37gLj"
      },
      "outputs": [],
      "source": [
        "# Extract the file based on its extension\n",
        "if zip_file_path.endswith('.zip'):\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(destination_dir)\n",
        "elif zip_file_path.endswith(('.tar', '.tar.gz', '.tgz')):\n",
        "    with tarfile.open(zip_file_path, 'r') as tar_ref:\n",
        "        tar_ref.extractall(destination_dir)\n",
        "else:\n",
        "    print(f\"Unsupported file extension: {zip_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4okFZMBR7ksH"
      },
      "outputs": [],
      "source": [
        "# Define the dataset directory\n",
        "dataset_dir = '/content/images/datasets_character_anime'  # Update if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZBVa2-K7s_m",
        "outputId": "c7fa3c84-4514-4dcc-f338-63f497b53676"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4043 images belonging to 43 classes.\n",
            "Found 1002 images belonging to 43 classes.\n"
          ]
        }
      ],
      "source": [
        "# Create an ImageDataGenerator for data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2  # Set the validation split if needed\n",
        ")\n",
        "\n",
        "# Create a generator for the training set\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "# Create a generator for the validation set\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7uKp_2dZxXi",
        "outputId": "34473ba6-38be-49dd-d3f6-cdab49f45900"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class names: ['Anya_Forger', 'Aquamarine_Hoshino', 'Ayaka_Genshin_Impact', 'Boa_Hancock', 'Charlotte_Genshin_Impact', 'Damian_Desmond', 'Dazai_Osamu_BSD', 'Ganyu_genshin', 'Gojo_Satoru', 'Jett_Valorant', 'Kafka_Honkai_Star_Rail', 'Keqing_genshin', 'Killjoy_Valorant', 'Kobo_kanaeru', 'Kugisaki_Nobara', 'Loid_Forger', 'Luffy_D_Monkey', 'Midoriya_Izuku', 'Minato_Aqua', 'Misa_Amane', 'Mitsuri_Kanroji', 'Nico_Robin', 'Nier_Automata_9S', 'Nier_Automata_A2', 'Raiden_Shogun_Genshin_Impact', 'Ruby_Hoshino', 'Sakura_Haruno', 'Todoroki_Shoto', 'Tokisaki_Kurumi', 'Uraraka_Ochako', 'Wanderer', 'Yor_Forger', 'Yuri_Briar', 'Zerotwo', 'amelia_watson', 'arima_kousei', 'gawr_gura', 'hoshino_ai', 'hutao_genshin', 'mikasa', 'tanjiro', 'violet_evergarden', 'zeta_hololive']\n"
          ]
        }
      ],
      "source": [
        "# Display class names\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "print(\"Class names:\", class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mwY2daAE8HXW"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "cnn = Sequential()\n",
        "cnn.add(Conv2D(32, (3, 3), input_shape=(224, 224, 3), activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(BatchNormalization())\n",
        "\n",
        "cnn.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(BatchNormalization())\n",
        "\n",
        "cnn.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(BatchNormalization())\n",
        "\n",
        "cnn.add(Flatten())\n",
        "cnn.add(Dense(512, activation='relu'))\n",
        "cnn.add(Dropout(0.5))\n",
        "cnn.add(Dense(43, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ONKLTN_CsrAx"
      },
      "outputs": [],
      "source": [
        "cnn.add(Dense(43, activation='softmax'))  # Assuming you have 43 classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mZr25Uec8MuS"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqF_yoPU8OKw",
        "outputId": "b88e7080-9bcd-4dc3-f8b9-c2929c809e60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 222, 222, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 111, 111, 32)      0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 111, 111, 32)      128       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 54, 54, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 54, 54, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 52, 52, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 26, 26, 128)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 26, 26, 128)       512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 86528)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               44302848  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 43)                22059     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 44419051 (169.45 MB)\n",
            "Trainable params: 44418603 (169.44 MB)\n",
            "Non-trainable params: 448 (1.75 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Print model summary\n",
        "cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nlqvd0be8Ug_",
        "outputId": "752a97c1-560b-4e75-e743-33632a350e50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "127/127 [==============================] - 364s 3s/step - loss: 15.6860 - accuracy: 0.0492 - val_loss: 11.7040 - val_accuracy: 0.0319\n",
            "Epoch 2/10\n",
            "127/127 [==============================] - 376s 3s/step - loss: 6.7763 - accuracy: 0.0524 - val_loss: 6.9249 - val_accuracy: 0.0329\n",
            "Epoch 3/10\n",
            "127/127 [==============================] - 368s 3s/step - loss: 4.4933 - accuracy: 0.0576 - val_loss: 6.2081 - val_accuracy: 0.0489\n",
            "Epoch 4/10\n",
            "127/127 [==============================] - 363s 3s/step - loss: 3.8946 - accuracy: 0.0653 - val_loss: 3.7567 - val_accuracy: 0.0479\n",
            "Epoch 5/10\n",
            "127/127 [==============================] - 371s 3s/step - loss: 3.7725 - accuracy: 0.0663 - val_loss: 3.7504 - val_accuracy: 0.0539\n",
            "Epoch 6/10\n",
            "127/127 [==============================] - 369s 3s/step - loss: 3.7317 - accuracy: 0.0732 - val_loss: 3.8574 - val_accuracy: 0.0649\n",
            "Epoch 7/10\n",
            "127/127 [==============================] - 376s 3s/step - loss: 3.6820 - accuracy: 0.0787 - val_loss: 3.6600 - val_accuracy: 0.0749\n",
            "Epoch 8/10\n",
            "127/127 [==============================] - 368s 3s/step - loss: 3.6612 - accuracy: 0.0789 - val_loss: 3.7107 - val_accuracy: 0.0669\n",
            "Epoch 9/10\n",
            "127/127 [==============================] - 378s 3s/step - loss: 3.6729 - accuracy: 0.0816 - val_loss: 3.6632 - val_accuracy: 0.0629\n",
            "Epoch 10/10\n",
            "127/127 [==============================] - 377s 3s/step - loss: 3.6398 - accuracy: 0.0858 - val_loss: 3.6625 - val_accuracy: 0.0739\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "history = cnn.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model_save_path = '/content/your_model.h5'  # Replace with the desired path and filename\n",
        "cnn.save(model_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5bbJnwoDRq9",
        "outputId": "684e7570-51fd-4dfd-827e-48d0216e054b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 265ms/step\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ACER\\Documents\\Documents\\Leon Folder\\ML-KYC-Feature\\imageclassification_1.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ACER/Documents/Documents/Leon%20Folder/ML-KYC-Feature/imageclassification_1.ipynb#X16sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Get the predicted class index\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ACER/Documents/Documents/Leon%20Folder/ML-KYC-Feature/imageclassification_1.ipynb#X16sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m predicted_class_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(result)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ACER/Documents/Documents/Leon%20Folder/ML-KYC-Feature/imageclassification_1.ipynb#X16sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m predicted_class \u001b[39m=\u001b[39m class_names[predicted_class_index]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ACER/Documents/Documents/Leon%20Folder/ML-KYC-Feature/imageclassification_1.ipynb#X16sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe predicted class is: \u001b[39m\u001b[39m{\u001b[39;00mpredicted_class\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Load the trained model\n",
        "model_path = 'new_anime_classification.h5'  # Replace with the actual path to your model\n",
        "loaded_model = load_model(model_path)\n",
        "\n",
        "# Function to preprocess the input image\n",
        "def preprocess_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=(128, 128))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    return img_array / 255.0\n",
        "\n",
        "# Function to make predictions\n",
        "def predict_image(model, img_path):\n",
        "    processed_image = preprocess_image(img_path)\n",
        "    prediction = model.predict(processed_image)\n",
        "    return prediction\n",
        "\n",
        "# Replace 'your_image.jpg' with the actual path to the image you want to test\n",
        "image_path = 'tanjiro.png'\n",
        "result = predict_image(loaded_model, image_path)\n",
        "# Assuming you have 36 classes, replace this with your actual class names\n",
        "class_names = ['amelia_watson', 'Anya_Forger', 'Aquamarine_Hoshino', 'arima_kousei', 'Ayaka_Genshin_Impact', 'Boa_Hancock', 'Charlotte_Genshin_Impact', 'Damian_Desmond', 'Dazai_Osamu_BSD', 'Ganyu_genshin', 'gawr_gura', 'Gojo_Satoru', 'hoshino_ai', 'hutao_genshin', 'Jett_Valorant', 'Kafka_Honkai_Star_Rail', 'Keqing_genshin', 'Killjoy_Valorant', 'Kobo_kanaeru', 'Kugisaki_Nobara', 'Loid_Forger', 'Luffy_D_Monkey', 'Midoriya_Izuku', 'mikasa', 'Minato_Aqua', 'Misa_Amane', 'Mitsuri_Kanroji', 'Nico_Robin', 'Nier_Automata_9S', 'Nier_Automata_A2', 'Raiden_Shogun_Genshin_Impact', 'Ruby_Hoshino', 'Sakura_Haruno', 'tanjiro', 'Todoroki_Shoto', 'Tokisaki_Kurumi', 'Uraraka_Ochako', 'violet_evergarden', 'Wanderer', 'Yor_Forger', 'Yuri_Briar', 'Zerotwo', 'zeta_hololive']\n",
        "\n",
        "# Get the predicted class index\n",
        "predicted_class_index = np.argmax(result)\n",
        "predicted_class = class_names[predicted_class_index]\n",
        "\n",
        "print(f\"The predicted class is: {predicted_class}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
